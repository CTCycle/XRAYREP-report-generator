{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# set warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category = Warning)\n",
    "\n",
    "# add parent folder path to the namespace\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..'))  \n",
    "\n",
    "# import modules and classes\n",
    "from utils.data_assets import PreProcessing, DataGenerator, TensorDataSet\n",
    "from utils.model_assets import ModelTraining, Inference\n",
    "import utils.global_paths as globpt\n",
    "import configurations as cnf\n",
    "\n",
    "# specify relative paths from global paths and create subfolders\n",
    "images_path = os.path.join(globpt.data_path, 'images') \n",
    "cp_path = os.path.join(globpt.train_path, 'checkpoints') \n",
    "os.mkdir(images_path) if not os.path.exists(images_path) else None\n",
    "os.mkdir(cp_path) if not os.path.exists(cp_path) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model and its parameters\n",
    "inference = Inference(cnf.seed) \n",
    "model, parameters = inference.load_pretrained_model(cp_path)\n",
    "model_path = inference.folder_path\n",
    "model.summary()\n",
    "\n",
    "# Load the tokenizer\n",
    "preprocessor = PreProcessing()\n",
    "preprocessing_path = os.path.join(model_path, 'preprocessing')\n",
    "tokenizer = preprocessor.load_tokenizer(preprocessing_path, 'word_tokenizer')\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# load preprocessed csv files (train and test datasets)\n",
    "file_loc = os.path.join(preprocessing_path, 'XREP_train.csv') \n",
    "train_data = pd.read_csv(file_loc, encoding = 'utf-8', sep = (';' or ',' or ' ' or  ':'), low_memory=False)\n",
    "file_loc = os.path.join(preprocessing_path, 'XREP_test.csv') \n",
    "test_data = pd.read_csv(file_loc, encoding = 'utf-8', sep = (';' or ',' or ' ' or  ':'), low_memory=False)\n",
    "\n",
    "# initialize training device\n",
    "trainer = ModelTraining(device=cnf.training_device, seed=cnf.seed)\n",
    "\n",
    "# initialize the images generator for the train and test data, and create the \n",
    "# tf.dataset according to batch shapes\n",
    "train_generator = DataGenerator(train_data, cnf.batch_size, cnf.picture_shape, \n",
    "                                shuffle=True, augmentation=cnf.augmentation)\n",
    "test_generator = DataGenerator(test_data, cnf.batch_size, cnf.picture_shape, \n",
    "                               shuffle=True, augmentation=cnf.augmentation)\n",
    "\n",
    "# initialize the TensorDataSet class with the generator instances\n",
    "# create the tf.datasets using the previously initialized generators \n",
    "datamaker = TensorDataSet()\n",
    "train_dataset = datamaker.create_tf_dataset(train_generator)\n",
    "test_dataset = datamaker.create_tf_dataset(test_generator)\n",
    "caption_shape = datamaker.y_batch.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Evaluation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = train_data.shape[0]\n",
    "num_test_samples = test_data.shape[0]\n",
    "\n",
    "print(f'''\n",
    "-------------------------------------------------------------------------------\n",
    "XRAYREP evaluation report\n",
    "-------------------------------------------------------------------------------\n",
    "Number of train samples: {num_train_samples}\n",
    "Number of test samples:  {num_test_samples}\n",
    "-------------------------------------------------------------------------------\n",
    "Batch size:              {cnf.batch_size}\n",
    "Epochs:                  {cnf.epochs}\n",
    "Vocabulary size:         {vocab_size + 1}\n",
    "Caption length:          {caption_shape} \n",
    "-------------------------------------------------------------------------------\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aquarius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
